{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<i>The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click </i> <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOC using a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LES Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D simulations were run for the BOMEX (nonprecipitating shallow cumulus) case by Peter Blossey. I was told the 3D output is hourly. I copied and extracted the archived data, and derived new datasets including only vertical velocity, liquid water potential temperature, and total water. Note that precipitating water is not counted for total water in this case, but this shouldn't matter (since this is a nonprecipitating shallow cumulus case). I also assume that all non-precipitating condensate is water (even though this output includes ice), with the reasoning that ice cloud should not be present in the BOMEX case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Liquid water potential temperature is determined using the approximation $$\\theta_l \\approx \\theta - \\frac{L_v}{c_{pd}} r_L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the latent heat of vaporizaton of water $L_v$ is taken to be its value at 0 Celcius (2.501 x 10^6 J/kg), and $\\theta$ is potential temperature, given by $$\\theta = T (\\frac{P_0}{P})^{R_d/{c_{pd}}}$$ where $P$ is pressure and $P_0$ is reference pressure (10^5 Pa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from subprocess import call\n",
    "from tempfile import TemporaryFile\n",
    "from glob import glob\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lv0 = 2.501e6  # Latent heat of vaporization for water at 0 Celsius (J/kg)\n",
    "Cpd = 1005.  # Specific heat of dry air at constant pressure (J/kg/K)\n",
    "\n",
    "def derive_hoc_dataset(original_dataset):\n",
    "    \"\"\"Takes in an xarray Dataset containing SAM 3D snapshot output, and returns a dataset\n",
    "       containing liquid water potential temperature, total water mixing ratio, and\n",
    "       vertical velocity in addition to the original variables.\n",
    "    \"\"\"\n",
    "    dataset = original_dataset.copy(deep=True)\n",
    "    theta = dataset['TABS'] * (1e3 / dataset['p']) ** (2./7.)  # SAM gives pressure in hPa\n",
    "    dataset['thetal'] = theta - Lv0/Cpd * dataset['QN'] * 1e-3  # SAM gives QN in g/kg\n",
    "    dataset['thetal'].attrs['units'] = 'K'\n",
    "    dataset['thetal'].attrs['long_name'] = 'liquid water potential temperature'\n",
    "    dataset['qt'] = dataset['Q'] + dataset['QN']\n",
    "    dataset['qt'].attrs['units'] = 'g/kg'\n",
    "    dataset['qt'].attrs['long_name'] = 'total water mixing ratio'\n",
    "    dataset.rename({'W': 'w'}, inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archive_folder = '/home/disk/eos1/bloss/Runs/BOMEX/DATA3D/'\n",
    "netcdf_folder = '/home/disk/eos4/mcgibbon/nobackup/HOC/data/BOMEX/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gzipped_filename in glob(archive_folder + '*sst_bmx3_16_*.nc.gz')[:20]:\n",
    "    netcdf_filename = gzipped_filename[:-3].replace(archive_folder, netcdf_folder)  # remove the '.gz', switch folder\n",
    "    if not os.path.isfile(netcdf_filename):  # check if the file has already been processed\n",
    "        with open(netcdf_filename, 'w') as output_file:\n",
    "            #print('Unzipping {} to {}'.format(gzipped_filename, netcdf_filename))\n",
    "            call(['gunzip', '-c', gzipped_filename], stdout=output_file)\n",
    "        with xr.open_dataset(netcdf_filename) as original_dataset:\n",
    "            new_dataset = derive_hoc_dataset(original_dataset)\n",
    "            new_dataset.drop(['U', 'V', 'TABS'])\n",
    "        new_dataset.to_netcdf(netcdf_filename, mode='w')  # overwrite the copied dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this pre-processing step complete, I derived a dataset with the statistical moments and parameters that will be used as input and outputs for the neural network. Input moments are the same as have been used to fit the Lewellen-Yoh joint double Gaussian PDF: $\\overline{w}$, $\\overline{w'^2}$, $\\overline{w'^3}$, $\\overline{\\theta_l}$, $\\overline{\\theta_l'^2}$, $\\overline{\\theta_l'^3}$, $\\overline{q_t}$, $\\overline{q_t'^2}$, $\\overline{q_t'^3}$, $\\overline{w'q_t'}$, $\\overline{w'\\theta_l'}$, and $\\overline{q_t'\\theta_l'}$. For this initial test, only mean liquid water, $\\overline{q_l}$, will be used as an output. SAM's \"nonprecipitating water\" is used as a measure of liquid water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_moments(array, n_max=3):\n",
    "    \"\"\"Takes in a DataArray and returns its statistical moments up to order n_max.\"\"\"\n",
    "    mean = array.mean(dim=('nx', 'ny'))\n",
    "    return_list = [mean]\n",
    "    perturbation = array - mean\n",
    "    for moment_order in range(1, n_max):\n",
    "        return_list.append((perturbation**moment_order).mean(dim=('nx', 'ny')))\n",
    "    return return_list\n",
    "\n",
    "def get_eddy_covariance(array1, array2):\n",
    "    \"\"\"Takes in two DataArrays and returns bar(array1' * array2')\"\"\"\n",
    "    mean1 = array1.mean(dim=('nx', 'ny'))\n",
    "    mean2 = array2.mean(dim=('nx', 'ny'))\n",
    "    perturbation1 = array1 - mean1\n",
    "    perturbation2 = array2 - mean2\n",
    "    return (perturbation1 * perturbation2).mean(dim=('nx', 'ny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snapshot_filenames = glob(netcdf_folder + '*sst_bmx3_16_*.nc')\n",
    "with xr.open_dataset(snapshot_filenames[0]) as sample_snapshot:\n",
    "    z = sample_snapshot['z'].copy(deep=True)\n",
    "\n",
    "ntime = len(snapshot_filenames)\n",
    "nz = len(z)\n",
    "time = np.zeros([ntime])\n",
    "\n",
    "base_vars = ['w', 'thetal', 'qt']\n",
    "\n",
    "output_moments = []\n",
    "for base_varname in base_vars:\n",
    "    # want variable names for the mean, second moment, and third moment\n",
    "    output_moments.extend([base_varname, base_varname + '2', base_varname + '3'])\n",
    "\n",
    "data = {}\n",
    "for varname in output_moments + ['w_qt', 'w_thetal', 'qt_thetal', 'ql']:\n",
    "    data[varname] = np.zeros([ntime, nz])\n",
    "\n",
    "for it, snapshot_filename in enumerate(snapshot_filenames):\n",
    "    with xr.open_dataset(snapshot_filename) as snapshot:\n",
    "        for base_varname in base_vars:\n",
    "            v, v2, v3 = get_moments(snapshot[base_varname], n_max=3)\n",
    "            data[base_varname][it, :] = v\n",
    "            data[base_varname + '2'][it, :] = v2\n",
    "            data[base_varname + '3'][it, :] = v3\n",
    "            time[it] = snapshot['time'][0]\n",
    "\n",
    "        data['w_qt'][it, :] = get_eddy_covariance(snapshot['w'], snapshot['qt'])\n",
    "        data['w_thetal'][it, :] = get_eddy_covariance(snapshot['w'], snapshot['thetal'])\n",
    "        data['qt_thetal'][it, :] = get_eddy_covariance(snapshot['qt'], snapshot['thetal'])\n",
    "        \n",
    "        data['ql'][it, :] = get_moments(snapshot['QN'], n_max=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords = {'time': (['time'], time), 'z': (['z'], z)}\n",
    "data_vars = {}\n",
    "for varname in data:\n",
    "    data_vars[varname] = (['time', 'z'], data[varname])\n",
    "ds = xr.Dataset(data_vars, coords=coords)\n",
    "ds.to_netcdf(os.path.join(netcdf_folder, 'moments.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was normalized to lie between 0 and 1, based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(os.path.join(netcdf_folder, 'moments.nc'))\n",
    "num_samples = len(ds['time'])*len(ds['z'])\n",
    "\n",
    "input_parameters = ['w', 'w2', 'w3', 'thetal', 'thetal2', 'thetal3', 'qt', 'qt2', 'qt3', 'w_thetal', 'w_qt', 'qt_thetal']\n",
    "output_parameters = ['ql']\n",
    "\n",
    "model_input = np.zeros([num_samples, len(input_parameters)])\n",
    "model_output = np.zeros([num_samples, len(output_parameters)])\n",
    "\n",
    "for i, input_parameter in enumerate(input_parameters):\n",
    "    model_input[:, i] = ds[input_parameter].values.reshape([num_samples])\n",
    "\n",
    "for i, output_parameter in enumerate(output_parameters):\n",
    "    model_output[:, i] = ds[output_parameter].values.reshape([num_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_normalization(data):\n",
    "    min_value = data.min(axis=0)\n",
    "    max_value = data.max(axis=0)\n",
    "    def normalize(data):\n",
    "        return (data - min_value)/(max_value - min_value)\n",
    "    def denormalize(data):\n",
    "        return data * (max_value - min_value) + min_value\n",
    "    return normalize, denormalize\n",
    "\n",
    "normalize_input, denormalize_input = get_normalization(model_input)\n",
    "normalize_output, denormalize_output = get_normalization(model_output)\n",
    "model_input = normalize_input(model_input)\n",
    "model_output = normalize_output(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% of the data was used for training the model, while 20% was reserved for validation. Training data was chosen at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)  # seed so results are reproduceable\n",
    "\n",
    "total_samples = model_input.shape[0]\n",
    "training_proportion = 0.8\n",
    "training_samples = int(training_proportion*total_samples)\n",
    "training_indices = np.random.choice(range(total_samples), size=training_samples, replace=False)\n",
    "test_indices = list(range(total_samples))\n",
    "for index in training_indices:\n",
    "    test_indices.remove(index)\n",
    "\n",
    "training_input = model_input[training_indices, :]\n",
    "training_output = model_output[training_indices, :]\n",
    "test_input = model_input[test_indices, :]\n",
    "test_output = model_output[test_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With statistical moments in hand, I defined a random forest to fit. Scikit-learn was used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(0)  # seed so results are reproduceable\n",
    "model = RandomForestRegressor(n_estimators=50).fit(training_input, training_output[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fit the random forest to the training data, I tested it on the testing data and the training data. Below we see a histogram of the model error on the test data (left) and the training data (right). Looking at the results from the testing data, the model error in general is quite small. From the frequency plot further below we can see that typical values of in-cloud liquid water are 5-8 x 10^-3 g/kg, while model errors are almost entirely below 0.6 x 10^-3 g/kg. It remains to be seen whether this is an improvement over other HOC approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeled_test_values = denormalize_output(model.predict(test_input))\n",
    "actual_test_values = denormalize_output(test_output)\n",
    "modeled_training_values = denormalize_output(model.predict(training_input))\n",
    "actual_training_values = denormalize_output(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 2))\n",
    "ax[0].hist(modeled_test_values - actual_test_values[:, 0], bins=np.linspace(-0.002, 0.002, 40))\n",
    "ax[0].set_xlabel('Modeled - Actual')\n",
    "ax[0].set_title('Test Data')\n",
    "ax[0].set_xlim(-0.002, 0.002)\n",
    "\n",
    "ax[1].hist(modeled_training_values - actual_training_values[:, 0], bins=np.linspace(-0.002, 0.002, 40))\n",
    "ax[1].set_xlabel('Modeled - Actual')\n",
    "ax[1].set_title('Training Data')\n",
    "ax[1].set_xlim(-0.002, 0.002)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_errors = modeled_test_values - actual_test_values[:, 0]\n",
    "training_errors = modeled_training_values - actual_training_values[:, 0]\n",
    "\n",
    "print('Test set error, mean: {:.2e}, min: {:.2e}, max: {:.2e}, RMSE: {:.2e}'.format(\n",
    "        test_errors.mean(), test_errors.min(), test_errors.max(), test_errors.std()))\n",
    "print('Training set error, mean: {:.2e}, min: {:.2e}, max: {:.2e}, RMSE: {:.2e}'.format(\n",
    "        training_errors.mean(), training_errors.min(),\n",
    "        training_errors.max(), training_errors.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the real and modeled PDF of liquid water, we see that the model reasonably represents the structure of the PDF. The model does perform better on the training data than the testing data, but the discrepancy is not large enough to suggest over-fitting. There are significant outliers (on the order of 3-4 g/kg), but one should keep in mind that due to the nature of the random forest, these are all within physical ranges. It is impossible for the forest to give a negative value, or a value greater than the largest value attained in the LES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(12, 7))\n",
    "ax[0, 0].hist(actual_test_values, bins=np.linspace(0, 0.02, 40))\n",
    "ax[0, 0].set_title('Testing actual values')\n",
    "\n",
    "ax[0, 1].hist(modeled_test_values, bins=np.linspace(0, 0.02, 40))\n",
    "ax[0, 1].set_title('Testing modeled values')\n",
    "\n",
    "ax[1, 0].hist(actual_training_values, bins=np.linspace(0, 0.02, 40))\n",
    "ax[1, 0].set_title('Training actual values')\n",
    "\n",
    "ax[1, 1].hist(modeled_training_values, bins=np.linspace(0, 0.02, 40))\n",
    "ax[1, 1].set_title('Training modeled values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a cell that contains the full process of defining, fitting, and evaluating a random forest model. This is here so that sensitivity to model parameters can be manually tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=10, max_depth=None, min_samples_leaf=4)\n",
    "model.fit(training_input, training_output[:, 0])\n",
    "modeled_test_values = denormalize_output(model.predict(test_input))\n",
    "actual_test_values = denormalize_output(test_output)\n",
    "modeled_training_values = denormalize_output(model.predict(training_input))\n",
    "actual_training_values = denormalize_output(training_output)\n",
    "\n",
    "test_errors = modeled_test_values - actual_test_values[:, 0]\n",
    "training_errors = modeled_training_values - actual_training_values[:, 0]\n",
    "\n",
    "print('Test set error, mean: {:.2e}, min: {:.2e}, max: {:.2e}, RMSE: {:.2e}'.format(\n",
    "        test_errors.mean(), test_errors.min(), test_errors.max(), test_errors.std()))\n",
    "print('Training set error, mean: {:.2e}, min: {:.2e}, max: {:.2e}, RMSE: {:.2e}'.format(\n",
    "        training_errors.mean(), training_errors.min(),\n",
    "        training_errors.max(), training_errors.std()))\n",
    "\n",
    "v_min, v_max = -0.001, 0.001\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 2))\n",
    "ax[0].hist(test_errors, bins=np.linspace(v_min, v_max, 40))\n",
    "ax[0].set_xlabel('Modeled - Actual')\n",
    "ax[0].set_title('Test Data')\n",
    "ax[0].set_xlim(v_min, v_max)\n",
    "\n",
    "ax[1].hist(training_errors, bins=np.linspace(v_min, v_max, 40))\n",
    "ax[1].set_xlabel('Modeled - Actual')\n",
    "ax[1].set_title('Training Data')\n",
    "ax[1].set_xlim(v_min, v_max)\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(12, 7))\n",
    "ax[0, 0].hist(actual_test_values, bins=np.linspace(0, 0.02, 50))\n",
    "ax[0, 0].set_title('Testing actual values')\n",
    "\n",
    "ax[0, 1].hist(modeled_test_values, bins=np.linspace(0, 0.02, 50))\n",
    "ax[0, 1].set_title('Testing modeled values')\n",
    "\n",
    "ax[1, 0].hist(actual_training_values, bins=np.linspace(0, 0.02, 50))\n",
    "ax[1, 0].set_title('Training actual values')\n",
    "\n",
    "ax[1, 1].hist(modeled_training_values, bins=np.linspace(0, 0.02, 50))\n",
    "ax[1, 1].set_title('Training modeled values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
